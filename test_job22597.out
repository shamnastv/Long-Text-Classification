comet_ml is installed but `COMET_API_KEY` is not set.
Namespace(batch_size=8, device=0, dropout=0.5, early_stop=10, epochs=100, hidden_dim=100, lr=0.001, num_layers=1, seed=0, weight_decay=1e-06)
device :  cuda:0
train size : 7631 7631
dev size : 847 847
test size : 3000
Model(
  (sent_encoder): SentenceGRU(
    (gru): GRU(768, 100, batch_first=True, dropout=0.5, bidirectional=True)
    (linear1): Linear(in_features=200, out_features=100, bias=True)
    (linear2): Linear(in_features=200, out_features=100, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (attention): Attention(
      (linears): ModuleList(
        (0): Linear(in_features=200, out_features=200, bias=False)
        (1): Linear(in_features=200, out_features=1, bias=False)
      )
    )
  )
  (doc_embed): DocumentGRU(
    (gru): GRU(100, 100, dropout=0.5, bidirectional=True)
    (linear1): Linear(in_features=200, out_features=100, bias=True)
    (linear2): Linear(in_features=200, out_features=100, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (attention): Attention(
      (linears): ModuleList(
        (0): Linear(in_features=200, out_features=200, bias=False)
        (1): Linear(in_features=200, out_features=1, bias=False)
      )
    )
  )
  (classifier): Linear(in_features=100, out_features=2, bias=True)
  (sent_norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (doc_norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    weight_decay: 1e-06
)
accuracy train: 0.468484 val: 0.458087, max val: 0.458087
Epoch :  1 loss_accum training:  388.7813869602978 Time :  106
accuracy train: 0.864500 val: 0.842975, max val: 0.842975

Epoch :  2 loss_accum training:  323.8389691449702 Time :  204
accuracy train: 0.866073 val: 0.851240, max val: 0.851240

Epoch :  3 loss_accum training:  305.1699206493795 Time :  302
accuracy train: 0.881536 val: 0.853601, max val: 0.853601

Epoch :  4 loss_accum training:  282.48732534796 Time :  399
accuracy train: 0.891757 val: 0.855962, max val: 0.855962

Epoch :  5 loss_accum training:  260.9317460730672 Time :  498
accuracy train: 0.907876 val: 0.841795, max val: 0.855962

Epoch :  6 loss_accum training:  228.86978600081056 Time :  573
accuracy train: 0.929760 val: 0.858323, max val: 0.858323

Epoch :  7 loss_accum training:  208.2658611536026 Time :  656
accuracy train: 0.932905 val: 0.829988, max val: 0.858323

Epoch :  8 loss_accum training:  170.98906264291145 Time :  740
accuracy train: 0.964618 val: 0.852420, max val: 0.858323

Epoch :  9 loss_accum training:  134.91221852996387 Time :  827
accuracy train: 0.977067 val: 0.844156, max val: 0.858323

Epoch :  10 loss_accum training:  109.09844515670557 Time :  912
accuracy train: 0.978902 val: 0.834711, max val: 0.858323

Epoch :  11 loss_accum training:  77.89243414485827 Time :  997
accuracy train: 0.984799 val: 0.798111, max val: 0.858323

Epoch :  12 loss_accum training:  70.73382851702627 Time :  1082
accuracy train: 0.987158 val: 0.855962, max val: 0.858323

Epoch :  13 loss_accum training:  52.599220420524944 Time :  1166
accuracy train: 0.992137 val: 0.829988, max val: 0.858323

Epoch :  14 loss_accum training:  47.663083128631115 Time :  1250
accuracy train: 0.993186 val: 0.828808, max val: 0.858323

Epoch :  15 loss_accum training:  39.58762117859442 Time :  1335
accuracy train: 0.995938 val: 0.837072, max val: 0.858323

Epoch :  16 loss_accum training:  33.7106127149309 Time :  1418
accuracy train: 0.997903 val: 0.831169, max val: 0.858323

Epoch :  17 loss_accum training:  28.212207413977012 Time :  1495
accuracy train: 0.996462 val: 0.839433, max val: 0.858323
/home/shamnast/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

output saved
comet_ml is installed but `COMET_API_KEY` is not set.
Namespace(batch_size=8, device=0, dropout=0.5, early_stop=10, epochs=100, hidden_dim=50, lr=0.001, num_layers=1, seed=0, weight_decay=0.001)
device :  cuda:0
train size : 7631 7631
dev size : 847 847
test size : 3000
Model(
  (sent_encoder): SentenceGRU(
    (gru): GRU(768, 50, batch_first=True, dropout=0.5, bidirectional=True)
    (linear1): Linear(in_features=100, out_features=50, bias=True)
    (linear2): Linear(in_features=100, out_features=50, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (attention): Attention(
      (linears): ModuleList(
        (0): Linear(in_features=100, out_features=100, bias=False)
        (1): Linear(in_features=100, out_features=1, bias=False)
      )
    )
  )
  (doc_embed): DocumentGRU(
    (gru): GRU(50, 50, dropout=0.5, bidirectional=True)
    (linear1): Linear(in_features=100, out_features=50, bias=True)
    (linear2): Linear(in_features=100, out_features=50, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (attention): Attention(
      (linears): ModuleList(
        (0): Linear(in_features=100, out_features=100, bias=False)
        (1): Linear(in_features=100, out_features=1, bias=False)
      )
    )
  )
  (classifier): Linear(in_features=50, out_features=2, bias=True)
  (sent_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (doc_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    weight_decay: 0.001
)
accuracy train: 0.457214 val: 0.443920, max val: 0.443920
Epoch :  1 loss_accum training:  401.4425101801753 Time :  92
accuracy train: 0.863583 val: 0.848878, max val: 0.848878

Epoch :  2 loss_accum training:  335.0137653425336 Time :  190
accuracy train: 0.866859 val: 0.858323, max val: 0.858323

Epoch :  3 loss_accum training:  306.7582501359284 Time :  288
accuracy train: 0.860176 val: 0.851240, max val: 0.858323

Epoch :  4 loss_accum training:  299.03666908852756 Time :  376
accuracy train: 0.885074 val: 0.859504, max val: 0.859504

Epoch :  5 loss_accum training:  288.07521971780807 Time :  475
accuracy train: 0.883108 val: 0.838253, max val: 0.859504

Epoch :  6 loss_accum training:  273.18089810572565 Time :  562
accuracy train: 0.895427 val: 0.852420, max val: 0.859504

Epoch :  7 loss_accum training:  259.37604689691216 Time :  650
accuracy train: 0.903944 val: 0.870130, max val: 0.870130

Epoch :  8 loss_accum training:  245.09796913247555 Time :  746
accuracy train: 0.906565 val: 0.868949, max val: 0.870130

Epoch :  9 loss_accum training:  237.13580098142847 Time :  833
accuracy train: 0.912987 val: 0.860685, max val: 0.870130

Epoch :  10 loss_accum training:  222.52156059071422 Time :  916
accuracy train: 0.925698 val: 0.858323, max val: 0.870130

Epoch :  11 loss_accum training:  210.40932300314307 Time :  1000
accuracy train: 0.916787 val: 0.866588, max val: 0.870130

Epoch :  12 loss_accum training:  197.39495246484876 Time :  1085
accuracy train: 0.922815 val: 0.867769, max val: 0.870130

Epoch :  13 loss_accum training:  183.84312401595525 Time :  1172
accuracy train: 0.926222 val: 0.871311, max val: 0.871311

Epoch :  14 loss_accum training:  173.49587748548947 Time :  1271
accuracy train: 0.945092 val: 0.860685, max val: 0.871311

Epoch :  15 loss_accum training:  161.49394629290327 Time :  1361
accuracy train: 0.963832 val: 0.840614, max val: 0.871311

Epoch :  16 loss_accum training:  140.12072256428655 Time :  1452
accuracy train: 0.957804 val: 0.863046, max val: 0.871311

Epoch :  17 loss_accum training:  131.64898602227913 Time :  1543
accuracy train: 0.965666 val: 0.847698, max val: 0.871311

Epoch :  18 loss_accum training:  115.81760661758017 Time :  1636
accuracy train: 0.957280 val: 0.818182, max val: 0.871311

Epoch :  19 loss_accum training:  102.1275409659429 Time :  1725
accuracy train: 0.972088 val: 0.834711, max val: 0.871311

Epoch :  20 loss_accum training:  91.30564735422377 Time :  1811
accuracy train: 0.954921 val: 0.800472, max val: 0.871311

Epoch :  21 loss_accum training:  84.52413763840741 Time :  1900
accuracy train: 0.986371 val: 0.851240, max val: 0.871311

Epoch :  22 loss_accum training:  74.33440296133631 Time :  1987
accuracy train: 0.991220 val: 0.855962, max val: 0.871311

Epoch :  23 loss_accum training:  59.93294250257895 Time :  2074
accuracy train: 0.980736 val: 0.841795, max val: 0.871311

Epoch :  24 loss_accum training:  57.062251066277895 Time :  2161
accuracy train: 0.984144 val: 0.863046, max val: 0.871311
/home/shamnast/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

output saved
comet_ml is installed but `COMET_API_KEY` is not set.
Namespace(batch_size=8, device=0, dropout=0.5, early_stop=10, epochs=100, hidden_dim=50, lr=0.005, num_layers=1, seed=0, weight_decay=0.0001)
device :  cuda:0
train size : 7631 7631
dev size : 847 847
test size : 3000
Model(
  (sent_encoder): SentenceGRU(
    (gru): GRU(768, 50, batch_first=True, dropout=0.5, bidirectional=True)
    (linear1): Linear(in_features=100, out_features=50, bias=True)
    (linear2): Linear(in_features=100, out_features=50, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (attention): Attention(
      (linears): ModuleList(
        (0): Linear(in_features=100, out_features=100, bias=False)
        (1): Linear(in_features=100, out_features=1, bias=False)
      )
    )
  )
  (doc_embed): DocumentGRU(
    (gru): GRU(50, 50, dropout=0.5, bidirectional=True)
    (linear1): Linear(in_features=100, out_features=50, bias=True)
    (linear2): Linear(in_features=100, out_features=50, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (attention): Attention(
      (linears): ModuleList(
        (0): Linear(in_features=100, out_features=100, bias=False)
        (1): Linear(in_features=100, out_features=1, bias=False)
      )
    )
  )
  (classifier): Linear(in_features=50, out_features=2, bias=True)
  (sent_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (doc_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.005
    lr: 0.005
    weight_decay: 0.0001
)
accuracy train: 0.457214 val: 0.443920, max val: 0.443920
Epoch :  1 loss_accum training:  443.94753366336226 Time :  106
accuracy train: 0.851527 val: 0.837072, max val: 0.837072

Epoch :  2 loss_accum training:  404.37540919333696 Time :  207
accuracy train: 0.837112 val: 0.820543, max val: 0.837072

Epoch :  3 loss_accum training:  390.98270212858915 Time :  296
accuracy train: 0.850871 val: 0.839433, max val: 0.839433

Epoch :  4 loss_accum training:  389.7570399418473 Time :  395
accuracy train: 0.854410 val: 0.833530, max val: 0.839433

Epoch :  5 loss_accum training:  383.58946398925036 Time :  486
accuracy train: 0.854803 val: 0.840614, max val: 0.840614

Epoch :  6 loss_accum training:  376.9972182288766 Time :  586
accuracy train: 0.854934 val: 0.835891, max val: 0.840614

Epoch :  7 loss_accum training:  369.74728997051716 Time :  673
accuracy train: 0.850478 val: 0.844156, max val: 0.844156

Epoch :  8 loss_accum training:  365.20487488247454 Time :  770
accuracy train: 0.855458 val: 0.833530, max val: 0.844156

Epoch :  9 loss_accum training:  365.147797530517 Time :  855
accuracy train: 0.851002 val: 0.839433, max val: 0.844156

Epoch :  10 loss_accum training:  362.535077849403 Time :  941
accuracy train: 0.864238 val: 0.837072, max val: 0.844156

Epoch :  11 loss_accum training:  350.395771170035 Time :  1029
accuracy train: 0.862665 val: 0.845336, max val: 0.845336

Epoch :  12 loss_accum training:  342.3212964423001 Time :  1129
accuracy train: 0.866204 val: 0.841795, max val: 0.845336

Epoch :  13 loss_accum training:  343.00984243676066 Time :  1217
accuracy train: 0.862928 val: 0.847698, max val: 0.847698

Epoch :  14 loss_accum training:  347.2688365597278 Time :  1316
accuracy train: 0.862534 val: 0.832349, max val: 0.847698

Epoch :  15 loss_accum training:  337.75242300517857 Time :  1398
accuracy train: 0.867907 val: 0.851240, max val: 0.851240

Epoch :  16 loss_accum training:  344.3010133188218 Time :  1484
accuracy train: 0.864107 val: 0.850059, max val: 0.851240

Epoch :  17 loss_accum training:  328.2810103101656 Time :  1562
accuracy train: 0.874459 val: 0.854782, max val: 0.854782

Epoch :  18 loss_accum training:  330.43433855101466 Time :  1679
accuracy train: 0.864500 val: 0.846517, max val: 0.854782

Epoch :  19 loss_accum training:  325.89495219569653 Time :  1785
accuracy train: 0.873149 val: 0.857143, max val: 0.857143

Epoch :  20 loss_accum training:  323.45026571676135 Time :  1900
accuracy train: 0.876425 val: 0.858323, max val: 0.858323

Epoch :  21 loss_accum training:  323.610589325428 Time :  2002
accuracy train: 0.876294 val: 0.854782, max val: 0.858323

Epoch :  22 loss_accum training:  316.0539039373398 Time :  2094
accuracy train: 0.867383 val: 0.842975, max val: 0.858323

Epoch :  23 loss_accum training:  320.61452267691493 Time :  2196
accuracy train: 0.877867 val: 0.851240, max val: 0.858323

Epoch :  24 loss_accum training:  315.86141744442284 Time :  2291
accuracy train: 0.876556 val: 0.848878, max val: 0.858323

Epoch :  25 loss_accum training:  308.153263553977 Time :  2393
accuracy train: 0.889792 val: 0.850059, max val: 0.858323

Epoch :  26 loss_accum training:  295.40585212549195 Time :  2501
accuracy train: 0.886384 val: 0.857143, max val: 0.858323

Epoch :  27 loss_accum training:  294.32984904479235 Time :  2606
accuracy train: 0.894509 val: 0.859504, max val: 0.859504

Epoch :  28 loss_accum training:  289.24397773854434 Time :  2723
accuracy train: 0.879701 val: 0.858323, max val: 0.859504

Epoch :  29 loss_accum training:  286.6915493160486 Time :  2829
accuracy train: 0.892413 val: 0.858323, max val: 0.859504

Epoch :  30 loss_accum training:  276.49220501817763 Time :  2936
accuracy train: 0.891757 val: 0.840614, max val: 0.859504

Epoch :  31 loss_accum training:  269.2792278639972 Time :  3042
accuracy train: 0.904338 val: 0.850059, max val: 0.859504

Epoch :  32 loss_accum training:  268.74686785787344 Time :  3145
accuracy train: 0.892150 val: 0.859504, max val: 0.859504

Epoch :  33 loss_accum training:  255.0351335350424 Time :  3251
accuracy train: 0.900406 val: 0.857143, max val: 0.859504

Epoch :  34 loss_accum training:  242.72155518550426 Time :  3357
accuracy train: 0.919408 val: 0.858323, max val: 0.859504

Epoch :  35 loss_accum training:  226.63630546350032 Time :  3463
accuracy train: 0.928712 val: 0.858323, max val: 0.859504

Epoch :  36 loss_accum training:  217.0153115130961 Time :  3567
accuracy train: 0.923994 val: 0.858323, max val: 0.859504

Epoch :  37 loss_accum training:  197.93053166382015 Time :  3672
accuracy train: 0.942340 val: 0.863046, max val: 0.863046

Epoch :  38 loss_accum training:  181.08909461600706 Time :  3791
accuracy train: 0.957411 val: 0.850059, max val: 0.863046

Epoch :  39 loss_accum training:  172.28812544699758 Time :  3890
accuracy train: 0.956100 val: 0.840614, max val: 0.863046

Epoch :  40 loss_accum training:  155.8743166103959 Time :  3998
accuracy train: 0.966453 val: 0.858323, max val: 0.863046

Epoch :  41 loss_accum training:  137.7828506093938 Time :  4109
accuracy train: 0.975102 val: 0.832349, max val: 0.863046

Epoch :  42 loss_accum training:  115.8372645971831 Time :  4220
accuracy train: 0.978247 val: 0.839433, max val: 0.863046

Epoch :  43 loss_accum training:  111.50784250861034 Time :  4332
accuracy train: 0.969467 val: 0.855962, max val: 0.863046

Epoch :  44 loss_accum training:  95.27515488374047 Time :  4446
accuracy train: 0.978771 val: 0.851240, max val: 0.863046

Epoch :  45 loss_accum training:  78.78437911032233 Time :  4557
accuracy train: 0.987813 val: 0.851240, max val: 0.863046

Epoch :  46 loss_accum training:  73.3169885739917 Time :  4668
accuracy train: 0.986502 val: 0.847698, max val: 0.863046

Epoch :  47 loss_accum training:  65.28455904178554 Time :  4781
accuracy train: 0.992399 val: 0.831169, max val: 0.863046

Epoch :  48 loss_accum training:  44.45335358433658 Time :  4887
accuracy train: 0.995151 val: 0.847698, max val: 0.863046
/home/shamnast/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

output saved
