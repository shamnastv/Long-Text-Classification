comet_ml is installed but `COMET_API_KEY` is not set.
Namespace(batch_size=8, device=0, dropout=0.5, early_stop=10, epochs=100, hidden_dim=50, lr=0.001, num_layers=1, seed=0, weight_decay=1e-06)
device :  cuda:0
train size : 7631 7631
dev size : 847 847
test size : 3000
Model(
  (sent_encoder): SentenceGRU(
    (gru): GRU(768, 50, batch_first=True, dropout=0.5, bidirectional=True)
    (linear1): Linear(in_features=100, out_features=50, bias=True)
    (linear2): Linear(in_features=100, out_features=50, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (attention): Attention(
      (linears): ModuleList(
        (0): Linear(in_features=100, out_features=100, bias=False)
        (1): Linear(in_features=100, out_features=1, bias=False)
      )
    )
  )
  (doc_embed): DocumentGRU(
    (gru): GRU(50, 50, dropout=0.5, bidirectional=True)
    (linear1): Linear(in_features=100, out_features=50, bias=True)
    (linear2): Linear(in_features=100, out_features=50, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (attention): Attention(
      (linears): ModuleList(
        (0): Linear(in_features=100, out_features=100, bias=False)
        (1): Linear(in_features=100, out_features=1, bias=False)
      )
    )
  )
  (classifier): Linear(in_features=50, out_features=2, bias=True)
  (sent_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (doc_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    weight_decay: 1e-06
)
accuracy train: 0.441882 val: 0.455726, max val: 0.455726
Epoch :  1 loss_accum training:  383.6489711254835 Time :  109
accuracy train: 0.868562 val: 0.852420, max val: 0.852420

Epoch :  2 loss_accum training:  311.06121654622257 Time :  199
accuracy train: 0.868431 val: 0.846517, max val: 0.852420

Epoch :  3 loss_accum training:  286.96595019474626 Time :  277
accuracy train: 0.896606 val: 0.858323, max val: 0.858323

Epoch :  4 loss_accum training:  261.8943268954754 Time :  366
accuracy train: 0.894378 val: 0.865407, max val: 0.865407

Epoch :  5 loss_accum training:  238.79495578352362 Time :  464
accuracy train: 0.920849 val: 0.844156, max val: 0.865407

Epoch :  6 loss_accum training:  205.81491452408955 Time :  553
accuracy train: 0.940506 val: 0.844156, max val: 0.865407

Epoch :  7 loss_accum training:  171.35381335555576 Time :  641
accuracy train: 0.958459 val: 0.846517, max val: 0.865407

Epoch :  8 loss_accum training:  132.17901869351044 Time :  730
accuracy train: 0.978247 val: 0.844156, max val: 0.865407

Epoch :  9 loss_accum training:  95.2827740412904 Time :  818
accuracy train: 0.982702 val: 0.847698, max val: 0.865407

Epoch :  10 loss_accum training:  68.87003638566239 Time :  905
accuracy train: 0.990303 val: 0.842975, max val: 0.865407

Epoch :  11 loss_accum training:  54.900009640696226 Time :  992
accuracy train: 0.994627 val: 0.841795, max val: 0.865407

Epoch :  12 loss_accum training:  43.15843542738003 Time :  1081
accuracy train: 0.996069 val: 0.848878, max val: 0.865407

Epoch :  13 loss_accum training:  32.04158231252222 Time :  1170
accuracy train: 0.997117 val: 0.844156, max val: 0.865407

Epoch :  14 loss_accum training:  26.4927835741546 Time :  1258
accuracy train: 0.997117 val: 0.854782, max val: 0.865407

Epoch :  15 loss_accum training:  27.122472978022415 Time :  1344
accuracy train: 0.995807 val: 0.835891, max val: 0.865407
/home/shamnast/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

output saved
comet_ml is installed but `COMET_API_KEY` is not set.
Namespace(batch_size=8, device=0, dropout=0.5, early_stop=10, epochs=100, hidden_dim=50, lr=0.0005, num_layers=1, seed=0, weight_decay=1e-06)
device :  cuda:0
train size : 7631 7631
dev size : 847 847
test size : 3000
Model(
  (sent_encoder): SentenceGRU(
    (gru): GRU(768, 50, batch_first=True, dropout=0.5, bidirectional=True)
    (linear1): Linear(in_features=100, out_features=50, bias=True)
    (linear2): Linear(in_features=100, out_features=50, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (attention): Attention(
      (linears): ModuleList(
        (0): Linear(in_features=100, out_features=100, bias=False)
        (1): Linear(in_features=100, out_features=1, bias=False)
      )
    )
  )
  (doc_embed): DocumentGRU(
    (gru): GRU(50, 50, dropout=0.5, bidirectional=True)
    (linear1): Linear(in_features=100, out_features=50, bias=True)
    (linear2): Linear(in_features=100, out_features=50, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (attention): Attention(
      (linears): ModuleList(
        (0): Linear(in_features=100, out_features=100, bias=False)
        (1): Linear(in_features=100, out_features=1, bias=False)
      )
    )
  )
  (classifier): Linear(in_features=50, out_features=2, bias=True)
  (sent_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (doc_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0005
    lr: 0.0005
    weight_decay: 1e-06
)
accuracy train: 0.441882 val: 0.455726, max val: 0.455726
Epoch :  1 loss_accum training:  386.49074425548315 Time :  109
accuracy train: 0.872756 val: 0.846517, max val: 0.846517

Epoch :  2 loss_accum training:  304.36770315282047 Time :  210
accuracy train: 0.870266 val: 0.847698, max val: 0.847698

Epoch :  3 loss_accum training:  278.8890435900539 Time :  305
accuracy train: 0.890447 val: 0.857143, max val: 0.857143

Epoch :  4 loss_accum training:  253.87512545939535 Time :  401
accuracy train: 0.899227 val: 0.859504, max val: 0.859504

Epoch :  5 loss_accum training:  226.28914440516382 Time :  499
accuracy train: 0.932774 val: 0.846517, max val: 0.859504

Epoch :  6 loss_accum training:  200.15526067093015 Time :  587
accuracy train: 0.948893 val: 0.840614, max val: 0.859504

Epoch :  7 loss_accum training:  165.7834137091413 Time :  672
accuracy train: 0.961735 val: 0.844156, max val: 0.859504

Epoch :  8 loss_accum training:  134.44546540710144 Time :  758
accuracy train: 0.971039 val: 0.852420, max val: 0.859504

Epoch :  9 loss_accum training:  104.28887128306087 Time :  845
accuracy train: 0.968680 val: 0.853601, max val: 0.859504

Epoch :  10 loss_accum training:  76.08623907441506 Time :  933
accuracy train: 0.987682 val: 0.851240, max val: 0.859504

Epoch :  11 loss_accum training:  63.49331109020568 Time :  1020
accuracy train: 0.992530 val: 0.848878, max val: 0.859504

Epoch :  12 loss_accum training:  51.50877313128149 Time :  1108
accuracy train: 0.993972 val: 0.838253, max val: 0.859504

Epoch :  13 loss_accum training:  45.89836409810232 Time :  1195
accuracy train: 0.991351 val: 0.850059, max val: 0.859504

Epoch :  14 loss_accum training:  31.58914875749906 Time :  1280
accuracy train: 0.995151 val: 0.844156, max val: 0.859504

Epoch :  15 loss_accum training:  23.799690025545715 Time :  1363
accuracy train: 0.998296 val: 0.857143, max val: 0.859504
/home/shamnast/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

output saved
comet_ml is installed but `COMET_API_KEY` is not set.
Namespace(batch_size=8, device=0, dropout=0.5, early_stop=10, epochs=100, hidden_dim=50, lr=0.0001, num_layers=1, seed=0, weight_decay=1e-06)
device :  cuda:0
train size : 7631 7631
dev size : 847 847
test size : 3000
Model(
  (sent_encoder): SentenceGRU(
    (gru): GRU(768, 50, batch_first=True, dropout=0.5, bidirectional=True)
    (linear1): Linear(in_features=100, out_features=50, bias=True)
    (linear2): Linear(in_features=100, out_features=50, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (attention): Attention(
      (linears): ModuleList(
        (0): Linear(in_features=100, out_features=100, bias=False)
        (1): Linear(in_features=100, out_features=1, bias=False)
      )
    )
  )
  (doc_embed): DocumentGRU(
    (gru): GRU(50, 50, dropout=0.5, bidirectional=True)
    (linear1): Linear(in_features=100, out_features=50, bias=True)
    (linear2): Linear(in_features=100, out_features=50, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (attention): Attention(
      (linears): ModuleList(
        (0): Linear(in_features=100, out_features=100, bias=False)
        (1): Linear(in_features=100, out_features=1, bias=False)
      )
    )
  )
  (classifier): Linear(in_features=50, out_features=2, bias=True)
  (sent_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (doc_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 0.0001
    weight_decay: 1e-06
)
accuracy train: 0.441882 val: 0.455726, max val: 0.455726
Epoch :  1 loss_accum training:  496.4597798958421 Time :  106
accuracy train: 0.854541 val: 0.834711, max val: 0.834711

Epoch :  2 loss_accum training:  328.3920956514776 Time :  200
accuracy train: 0.866728 val: 0.841795, max val: 0.841795

Epoch :  3 loss_accum training:  306.5859603434801 Time :  281
accuracy train: 0.874722 val: 0.840614, max val: 0.841795

Epoch :  4 loss_accum training:  287.6500599067658 Time :  352
accuracy train: 0.879439 val: 0.851240, max val: 0.851240

Epoch :  5 loss_accum training:  271.4430415984243 Time :  431
accuracy train: 0.891495 val: 0.845336, max val: 0.851240

Epoch :  6 loss_accum training:  261.93889910075814 Time :  502
accuracy train: 0.898703 val: 0.851240, max val: 0.851240

Epoch :  7 loss_accum training:  248.42534224735573 Time :  574
accuracy train: 0.900668 val: 0.850059, max val: 0.851240

Epoch :  8 loss_accum training:  240.4157704524696 Time :  645
accuracy train: 0.910890 val: 0.844156, max val: 0.851240

Epoch :  9 loss_accum training:  226.81424388475716 Time :  714
accuracy train: 0.913380 val: 0.850059, max val: 0.851240

Epoch :  10 loss_accum training:  216.46998702874407 Time :  782
accuracy train: 0.921373 val: 0.839433, max val: 0.851240

Epoch :  11 loss_accum training:  204.16606547031552 Time :  852
accuracy train: 0.929891 val: 0.846517, max val: 0.851240

Epoch :  12 loss_accum training:  196.61353560886346 Time :  924
accuracy train: 0.934478 val: 0.848878, max val: 0.851240

Epoch :  13 loss_accum training:  187.01206502201967 Time :  995
accuracy train: 0.931857 val: 0.845336, max val: 0.851240

Epoch :  14 loss_accum training:  174.1494743231451 Time :  1067
accuracy train: 0.949155 val: 0.847698, max val: 0.851240

Epoch :  15 loss_accum training:  163.60258020926267 Time :  1137
accuracy train: 0.952824 val: 0.845336, max val: 0.851240
/home/shamnast/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

output saved
comet_ml is installed but `COMET_API_KEY` is not set.
Namespace(batch_size=8, device=0, dropout=0.5, early_stop=10, epochs=100, hidden_dim=50, lr=5e-05, num_layers=1, seed=0, weight_decay=1e-06)
device :  cuda:0
train size : 7631 7631
dev size : 847 847
test size : 3000
Model(
  (sent_encoder): SentenceGRU(
    (gru): GRU(768, 50, batch_first=True, dropout=0.5, bidirectional=True)
    (linear1): Linear(in_features=100, out_features=50, bias=True)
    (linear2): Linear(in_features=100, out_features=50, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (attention): Attention(
      (linears): ModuleList(
        (0): Linear(in_features=100, out_features=100, bias=False)
        (1): Linear(in_features=100, out_features=1, bias=False)
      )
    )
  )
  (doc_embed): DocumentGRU(
    (gru): GRU(50, 50, dropout=0.5, bidirectional=True)
    (linear1): Linear(in_features=100, out_features=50, bias=True)
    (linear2): Linear(in_features=100, out_features=50, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (attention): Attention(
      (linears): ModuleList(
        (0): Linear(in_features=100, out_features=100, bias=False)
        (1): Linear(in_features=100, out_features=1, bias=False)
      )
    )
  )
  (classifier): Linear(in_features=50, out_features=2, bias=True)
  (sent_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (doc_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 5e-05
    lr: 5e-05
    weight_decay: 1e-06
)
accuracy train: 0.441882 val: 0.455726, max val: 0.455726
Epoch :  1 loss_accum training:  605.742635011673 Time :  89
accuracy train: 0.815883 val: 0.801653, max val: 0.801653

Epoch :  2 loss_accum training:  366.3053712323308 Time :  170
accuracy train: 0.856375 val: 0.838253, max val: 0.838253

Epoch :  3 loss_accum training:  329.6575000882149 Time :  250
accuracy train: 0.859651 val: 0.840614, max val: 0.840614

Epoch :  4 loss_accum training:  311.99272442795336 Time :  329
accuracy train: 0.864893 val: 0.845336, max val: 0.845336

Epoch :  5 loss_accum training:  297.40092358738184 Time :  410
accuracy train: 0.871576 val: 0.839433, max val: 0.845336

Epoch :  6 loss_accum training:  291.78451984748244 Time :  482
accuracy train: 0.875639 val: 0.841795, max val: 0.845336

Epoch :  7 loss_accum training:  279.6037171687931 Time :  554
accuracy train: 0.882322 val: 0.854782, max val: 0.854782

Epoch :  8 loss_accum training:  274.7269251458347 Time :  638
accuracy train: 0.887695 val: 0.847698, max val: 0.854782

Epoch :  9 loss_accum training:  265.80398261360824 Time :  713
accuracy train: 0.892019 val: 0.848878, max val: 0.854782

Epoch :  10 loss_accum training:  259.156836790964 Time :  788
accuracy train: 0.894247 val: 0.846517, max val: 0.854782

Epoch :  11 loss_accum training:  250.78856719983742 Time :  863
accuracy train: 0.899751 val: 0.850059, max val: 0.854782

Epoch :  12 loss_accum training:  245.6694597126916 Time :  938
accuracy train: 0.899227 val: 0.853601, max val: 0.854782

Epoch :  13 loss_accum training:  241.59272322710603 Time :  1013
accuracy train: 0.901979 val: 0.850059, max val: 0.854782

Epoch :  14 loss_accum training:  235.1384017141536 Time :  1084
accuracy train: 0.905517 val: 0.846517, max val: 0.854782

Epoch :  15 loss_accum training:  230.30213946383446 Time :  1154
accuracy train: 0.908269 val: 0.847698, max val: 0.854782

Epoch :  16 loss_accum training:  224.83273736946285 Time :  1223
accuracy train: 0.910235 val: 0.851240, max val: 0.854782

Epoch :  17 loss_accum training:  217.13761656917632 Time :  1292
accuracy train: 0.914559 val: 0.848878, max val: 0.854782

Epoch :  18 loss_accum training:  217.45269715879112 Time :  1364
accuracy train: 0.919015 val: 0.851240, max val: 0.854782
/home/shamnast/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

output saved
